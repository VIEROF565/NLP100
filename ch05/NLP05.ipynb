{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章 係り受け解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40.係り受け解析結果の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re\n",
    "fname = 'neko.txt'\n",
    "fname_parsed = 'neko.txt.cabocha'\n",
    "\n",
    "with open(fname) as data_file, open (fname_parsed,'w') as out_file:\n",
    "    cabocha = CaboCha.Parser()\n",
    "    for line in data_file:\n",
    "        out_file.write(cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self,surface,base,pos,pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(self.surface,self.base,self.pos,self.pos1)\n",
    "\n",
    "def neco_lines():\n",
    "    with open (fname_parsed) as file_parsed:\n",
    "        morphs = []\n",
    "        for line in file_parsed:\n",
    "            if line == 'EOS\\n':\n",
    "                yield morphs\n",
    "                morphs = []\n",
    "            else:\n",
    "                if line[0] == '*':\n",
    "                    continue\n",
    "                cols = line.split('\\t')\n",
    "                res_cols = cols[1].split(',')\n",
    "                morphs.append(Morph(\n",
    "                    cols[0],res_cols[6],res_cols[0],res_cols[1]))\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,morphs in enumerate(neco_lines(),1):\n",
    "    if i==3:\n",
    "        for morph in morphs:\n",
    "            print(morph)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41.係り受け解析結果の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.dst = -1\n",
    "        self.srcs = []\n",
    "    def __str__(self):\n",
    "        surface =''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface,self.srcs,self.dst)\n",
    "    \n",
    "    def normalized_surface(self):\n",
    "        result = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                result += morph.surface\n",
    "        return result\n",
    "    \n",
    "def neco_lines():\n",
    "    with open (fname_parsed) as file_parsed:\n",
    "        chunks = dict()\n",
    "        idx = -1\n",
    "        \n",
    "        for line in file_parsed:\n",
    "            if line == 'EOS\\n':\n",
    "                if len(chunks) > 0:\n",
    "                    sorted_tuple = sorted(chunks.items(), key = lambda x:x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]\n",
    "                    chunks.clear()\n",
    "                else:\n",
    "                    yield []\n",
    "            elif line[0] == '*': \n",
    "                cols = line.split(' ')\n",
    "                idx = int(cols[1])\n",
    "                dst = int(re.search(r'(.*?)D',cols[2]).group(1))\n",
    "                \n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "            else:\n",
    "                cols = line.split('\\t')\n",
    "                res_cols = cols[1].split(',')\n",
    "                chunks[idx].morphs.append(Morph(\n",
    "                    cols[0],res_cols[6],res_cols[0],res_cols[1]))\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chunks in enumerate(neco_lines(),1):\n",
    "    if i==8:\n",
    "        for j,chunk in enumerate(chunks):\n",
    "            print('[{}]{}'.format(j,chunk))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42.係り元と係り先の文節の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunks in neco_lines():\n",
    "    for chunk in chunks:\n",
    "        if chunk.dst != -1:\n",
    "            src = chunk.normalized_surface()\n",
    "            dst = chunks[chunk.dst].normalized_surface()\n",
    "            if src != '' and dst != '':\n",
    "                print('{}\\t{}'.format(src,dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43.名詞を含む文節が動詞を含む文節に係るものを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_V(morphs,hinsi):\n",
    "    for morph in morphs:\n",
    "        if morph.pos == hinsi:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "\n",
    "for chunks in neco_lines():\n",
    "    for chunk in chunks:\n",
    "        if chunk.dst != -1:\n",
    "            if check_V(chunk.morphs,'名詞') and check_V(chunks[chunk.dst].morphs,'動詞') :\n",
    "                src = chunk.normalized_surface()\n",
    "                dst = chunks[chunk.dst].normalized_surface()\n",
    "                if src != '' and dst != '':\n",
    "                    print('{}\\t{}'.format(src,dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44.係り受け解析の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re\n",
    "import pydot_ng as pydot\n",
    "\n",
    "fname = 'neko.txt.tmp'\n",
    "fname_parsed = 'neko.txt.cabocha.tmp'\n",
    "\n",
    "\n",
    "def parse_neko():\n",
    "    '''「吾輩は猫である」を係り受け解析\n",
    "    「吾輩は猫である」(neko.txt)を係り受け解析してneko.txt.cabochaに保存する\n",
    "    '''\n",
    "    with open(fname) as data_file, \\\n",
    "            open(fname_parsed, mode='w') as out_file:\n",
    "\n",
    "        cabocha = CaboCha.Parser()\n",
    "        for line in data_file:\n",
    "            out_file.write(\n",
    "                cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)\n",
    "            )\n",
    "\n",
    "\n",
    "class Morph:\n",
    "    '''\n",
    "    形態素クラス\n",
    "    表層形（surface）、基本形（base）、品詞（pos）、品詞細分類1（pos1）を\n",
    "    メンバー変数に持つ\n",
    "    '''\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        '''初期化'''\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __str__(self):\n",
    "        '''オブジェクトの文字列表現'''\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "\n",
    "class Chunk:\n",
    "    '''\n",
    "    文節クラス\n",
    "    形態素（Morphオブジェクト）のリスト（morphs）、係り先文節インデックス番号（dst）、\n",
    "    係り元文節インデックス番号のリスト（srcs）をメンバー変数に持つ\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''初期化'''\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "        self.dst = -1\n",
    "\n",
    "    def __str__(self):\n",
    "        '''オブジェクトの文字列表現'''\n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "\n",
    "    def normalized_surface(self):\n",
    "        '''句読点などの記号を除いた表層形'''\n",
    "        result = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                result += morph.surface\n",
    "        return result\n",
    "\n",
    "    def chk_pos(self, pos):\n",
    "        '''指定した品詞（pos）を含むかチェックする\n",
    "\n",
    "        戻り値：\n",
    "        品詞（pos）を含む場合はTrue\n",
    "        '''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def neco_lines():\n",
    "    '''「吾輩は猫である」の係り受け解析結果のジェネレータ\n",
    "    「吾輩は猫である」の係り受け解析結果を順次読み込んで、\n",
    "    1文ずつChunkクラスのリストを返す\n",
    "\n",
    "    戻り値：\n",
    "    1文のChunkクラスのリスト\n",
    "    '''\n",
    "    with open(fname_parsed) as file_parsed:\n",
    "\n",
    "        chunks = dict()     # idxをkeyにChunkを格納\n",
    "        idx = -1\n",
    "\n",
    "        for line in file_parsed:\n",
    "\n",
    "            # 1文の終了判定\n",
    "            if line == 'EOS\\n':\n",
    "\n",
    "                # Chunkのリストを返す\n",
    "                if len(chunks) > 0:\n",
    "\n",
    "                    # chunksをkeyでソートし、valueのみ取り出し\n",
    "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]\n",
    "                    chunks.clear()\n",
    "\n",
    "                else:\n",
    "                    yield []\n",
    "\n",
    "            # 先頭が*の行は係り受け解析結果なので、Chunkを作成\n",
    "            elif line[0] == '*':\n",
    "\n",
    "                # Chunkのインデックス番号と係り先のインデックス番号取得\n",
    "                cols = line.split(' ')\n",
    "                idx = int(cols[1])\n",
    "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
    "\n",
    "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "\n",
    "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "\n",
    "            # それ以外の行は形態素解析結果なので、Morphを作りChunkに追加\n",
    "            else:\n",
    "\n",
    "                # 表層形はtab区切り、それ以外は','区切りでバラす\n",
    "                cols = line.split('\\t')\n",
    "                res_cols = cols[1].split(',')\n",
    "\n",
    "                # Morph作成、リストに追加\n",
    "                chunks[idx].morphs.append(\n",
    "                    Morph(\n",
    "                        cols[0],        # surface\n",
    "                        res_cols[6],    # base\n",
    "                        res_cols[0],    # pos\n",
    "                        res_cols[1]     # pos1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "def graph_from_edges_ex(edge_list, directed=False):\n",
    "    '''pydot_ng.graph_from_edges()のノード識別子への対応版\n",
    "\n",
    "    graph_from_edges()のedge_listで指定するタプルは\n",
    "    識別子とグラフ表示時のラベルが同一のため、\n",
    "    ラベルが同じだが実体が異なるノードを表現することができない。\n",
    "    例えば文の係り受けをグラフにする際、文の中に同じ単語が\n",
    "    複数出てくると、それらのノードが同一視されて接続されてしまう。\n",
    "\n",
    "    この関数ではedge_listとして次の書式のタプルを受け取り、\n",
    "    ラベルが同一でも識別子が異なるノードは別ものとして扱う。\n",
    "\n",
    "    edge_list = [((識別子1,ラベル1),(識別子2,ラベル2)), ...]\n",
    "\n",
    "    識別子はノードを識別するためのもので表示されない。\n",
    "    ラベルは表示用で、同じでも識別子が異なれば別のノードになる。\n",
    "\n",
    "    なお、オリジナルの関数にあるnode_prefixは未実装。\n",
    "\n",
    "    戻り値：\n",
    "    pydot.Dotオブジェクト\n",
    "    '''\n",
    "\n",
    "    if directed:\n",
    "        graph = pydot.Dot(graph_type='digraph')\n",
    "\n",
    "    else:\n",
    "        graph = pydot.Dot(graph_type='graph')\n",
    "\n",
    "    for edge in edge_list:\n",
    "\n",
    "        id1 = str(edge[0][0])\n",
    "        label1 = str(edge[0][1])\n",
    "        id2 = str(edge[1][0])\n",
    "        label2 = str(edge[1][1])\n",
    "\n",
    "        # ノード追加\n",
    "        graph.add_node(pydot.Node(id1, label=label1))\n",
    "        graph.add_node(pydot.Node(id2, label=label2))\n",
    "\n",
    "        # エッジ追加\n",
    "        graph.add_edge(pydot.Edge(id1, id2))\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "# 対象文字列を入力してもらい、そのままfnameに保存\n",
    "with open(fname, mode='w') as out_file:\n",
    "    out_file.write(input('文字列を入力してください--> '))\n",
    "\n",
    "# 係り受け解析\n",
    "parse_neko()\n",
    "\n",
    "# 1文ずつリスト作成\n",
    "for chunks in neco_lines():\n",
    "\n",
    "    # 係り先があるものを列挙\n",
    "    edges = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if chunk.dst != -1:\n",
    "\n",
    "            # 記号を除いた表層形をチェック、空なら除外\n",
    "            src = chunk.normalized_surface()\n",
    "            dst = chunks[chunk.dst].normalized_surface()\n",
    "            if src != '' and dst != '':\n",
    "                edges.append(((i, src), (chunk.dst, dst)))\n",
    "\n",
    "    # 描画\n",
    "    if len(edges) > 0:\n",
    "        graph = graph_from_edges_ex(edges, directed=True)\n",
    "        graph.write_png('result.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45.動詞の格パターンの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re\n",
    "import pydot_ng as pydot\n",
    "\n",
    "fname = 'neko.txt'\n",
    "fname_parsed = 'neko.txt.cabocha'\n",
    "\n",
    "def parse_neko():\n",
    "    with open(fname) as data_file, \\\n",
    "            open(fname_parsed, mode='w') as out_file:\n",
    "\n",
    "        cabocha = CaboCha.Parser()\n",
    "        for line in data_file:\n",
    "            out_file.write(\n",
    "                cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)\n",
    "            )\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "        self.dst = -1\n",
    "\n",
    "    def __str__(self):\n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "\n",
    "    def normalized_surface(self):\n",
    "        result = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                result += morph.surface\n",
    "        return result\n",
    "\n",
    "    def chk_pos(self, pos):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_morphs_by_pos(self,pos,pos1=''):\n",
    "        if len(pos1) > 0:\n",
    "            return [res for res in self.morphs if (res.pos == pos) and (res.pos == pos1)]\n",
    "        else:\n",
    "            return [res for res in self.morphs if res.pos == pos]\n",
    "    \n",
    "def neco_lines():\n",
    "    with open(fname_parsed) as file_parsed:\n",
    "        chunks = dict() \n",
    "        idx = -1\n",
    "        for line in file_parsed:\n",
    "            if line == 'EOS\\n':\n",
    "                if len(chunks) > 0:\n",
    "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]\n",
    "                    chunks.clear()\n",
    "                else:\n",
    "                    yield []\n",
    "            elif line[0] == '*':\n",
    "                cols = line.split(' ')\n",
    "                idx = int(cols[1])\n",
    "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
    "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "            # それ以外の行は形態素解析結果なので、Morphを作りChunkに追加\n",
    "            else:\n",
    "                # 表層形はtab区切り、それ以外は','区切りでバラす\n",
    "                cols = line.split('\\t')\n",
    "                res_cols = cols[1].split(',')\n",
    "                # Morph作成、リストに追加\n",
    "                chunks[idx].morphs.append(\n",
    "                    Morph(\n",
    "                        cols[0],        # surface\n",
    "                        res_cols[6],    # base\n",
    "                        res_cols[0],    # pos\n",
    "                        res_cols[1]     # pos1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "parse_neko()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunks in neco_lines():\n",
    "    for chunk in chunks:\n",
    "        verbs = chunk.get_morphs_by_pos('動詞')\n",
    "        if len(verbs)< 1: continue\n",
    "        prts = []\n",
    "        for src in chunk.srcs:\n",
    "            prts_in_chunk = chunks[src].get_morphs_by_pos('助詞')\n",
    "            if len(prts_in_chunk) > 1:\n",
    "                kaku_prts = chunks[src].get_morphs_by_pos('助詞','格助詞')\n",
    "                prts_in_chunk = kaku_prts\n",
    "            if len(prts_in_chunk) > 0:\n",
    "                prts.append(prts_in_chunk[-1])\n",
    "        if len(prts) < 1:\n",
    "            continue\n",
    "        print(verbs[0].base,' '.join(sorted(prt.surface for prt in prts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46.動詞の格フレーム情報の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re\n",
    "import pydot_ng as pydot\n",
    "\n",
    "fname = 'neko.txt'\n",
    "fname_parsed = 'neko.txt.cabocha'\n",
    "\n",
    "def parse_neko():\n",
    "    with open(fname) as data_file, \\\n",
    "            open(fname_parsed, mode='w') as out_file:\n",
    "\n",
    "        cabocha = CaboCha.Parser()\n",
    "        for line in data_file:\n",
    "            out_file.write(\n",
    "                cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)\n",
    "            )\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "        self.dst = -1\n",
    "\n",
    "    def __str__(self):\n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "\n",
    "    def normalized_surface(self):\n",
    "        result = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                result += morph.surface\n",
    "        return result\n",
    "\n",
    "    def chk_pos(self, pos):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_morphs_by_pos(self,pos,pos1=''):\n",
    "        if len(pos1) > 0:\n",
    "            return [res for res in self.morphs if (res.pos == pos) and (res.pos == pos1)]\n",
    "        else:\n",
    "            return [res for res in self.morphs if res.pos == pos]\n",
    "    \n",
    "def neco_lines():\n",
    "    with open(fname_parsed) as file_parsed:\n",
    "        chunks = dict() \n",
    "        idx = -1\n",
    "        for line in file_parsed:\n",
    "            if line == 'EOS\\n':\n",
    "                if len(chunks) > 0:\n",
    "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]\n",
    "                    chunks.clear()\n",
    "                else:\n",
    "                    yield []\n",
    "            elif line[0] == '*':\n",
    "                cols = line.split(' ')\n",
    "                idx = int(cols[1])\n",
    "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "            else:\n",
    "                cols = line.split('\\t')\n",
    "                res_cols = cols[1].split(',')\n",
    "                chunks[idx].morphs.append(\n",
    "                    Morph(\n",
    "                        cols[0],        # surface\n",
    "                        res_cols[6],    # base\n",
    "                        res_cols[0],    # pos\n",
    "                        res_cols[1]     # pos1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "parse_neko()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunks in neco_lines():\n",
    "    for chunk in chunks:\n",
    "        verbs = chunk.get_morphs_by_pos('動詞')\n",
    "        if len(verbs)< 1: continue\n",
    "        prts = []\n",
    "        frame = []\n",
    "        for src in chunk.srcs:\n",
    "            prts_in_chunk = chunks[src].get_morphs_by_pos('助詞')\n",
    "            if len(prts_in_chunk) > 1:\n",
    "                kaku_prts = chunks[src].get_morphs_by_pos('助詞','格助詞')\n",
    "                prts_in_chunk = kaku_prts\n",
    "            if len(prts_in_chunk) > 0:\n",
    "                prts.append(prts_in_chunk[-1])\n",
    "                frame.append(chunks[src].normalized_surface())\n",
    "        if len(prts) < 1:\n",
    "            continue\n",
    "        print(verbs[0].base,' '.join(sorted(prt.surface for prt in prts)))\n",
    "        print(\":::\",frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47.機能動詞構文のマイニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re\n",
    "import pydot_ng as pydot\n",
    "\n",
    "fname = 'neko.txt'\n",
    "fname_parsed = 'neko.txt.cabocha'\n",
    "\n",
    "def parse_neko():\n",
    "    with open(fname) as data_file, \\\n",
    "            open(fname_parsed, mode='w') as out_file:\n",
    "\n",
    "        cabocha = CaboCha.Parser()\n",
    "        for line in data_file:\n",
    "            out_file.write(\n",
    "                cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)\n",
    "            )\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "        self.dst = -1\n",
    "\n",
    "    def __str__(self):\n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "\n",
    "    def normalized_surface(self):\n",
    "        result = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                result += morph.surface\n",
    "        return result\n",
    "\n",
    "    def chk_pos(self, pos):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_morphs_by_pos(self,pos,pos1=''):\n",
    "        if len(pos1) > 0:\n",
    "            return [res for res in self.morphs if (res.pos == pos) and (res.pos == pos1)]\n",
    "        else:\n",
    "            return [res for res in self.morphs if res.pos == pos]\n",
    "    \n",
    "    def get_kaku_prt(self):\n",
    "        prts = self.get_morphs_by_pos('助詞')\n",
    "        if len(prts) >1:\n",
    "            kaku_prts = self.get_morphs_by_pos('助詞','格助詞')\n",
    "            if len(kaku_prts) > 0:\n",
    "                prts[-1].surface\n",
    "        \n",
    "        if len(prts) > 0:\n",
    "            return prts[-1].surface\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    def get_sahen_wo(self):\n",
    "        for i,morph in enumerate(self.morphs[0:-1]) :\n",
    "            if(morph.pos == '名詞')\\\n",
    "              and (morph.pos1 == 'サ変接続')\\\n",
    "              and (self.morphs[i+1].pos == \"助詞\")\\\n",
    "              and (self.morphs[i +1].surface == 'を'):\n",
    "                return morph.surface + self.morphs[i + 1].surface\n",
    "        return ''\n",
    "    \n",
    "def neco_lines():\n",
    "    with open(fname_parsed) as file_parsed:\n",
    "        chunks = dict() \n",
    "        idx = -1\n",
    "        for line in file_parsed:\n",
    "            if line == 'EOS\\n':\n",
    "                if len(chunks) > 0:\n",
    "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]\n",
    "                    chunks.clear()\n",
    "                else:\n",
    "                    yield []\n",
    "            elif line[0] == '*':\n",
    "                cols = line.split(' ')\n",
    "                idx = int(cols[1])\n",
    "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "            else:\n",
    "                cols = line.split('\\t')\n",
    "                res_cols = cols[1].split(',')\n",
    "                chunks[idx].morphs.append(\n",
    "                    Morph(\n",
    "                        cols[0],        # surface\n",
    "                        res_cols[6],    # base\n",
    "                        res_cols[0],    # pos\n",
    "                        res_cols[1]     # pos1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "parse_neko()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunks in neco_lines():\n",
    "    for chunk in chunks:\n",
    "        tmpret = chunk.get_sahen_wo()\n",
    "        ret2 = ''\n",
    "        ret3 = ''\n",
    "        if len(tmpret) > 0 and chunks[chunk.dst].chk_pos('動詞'):\n",
    "            x = chunks[chunk.dst].get_morphs_by_pos('動詞')[0]\n",
    "            ret = tmpret + x.base\n",
    "            for lis in chunks[chunk.dst].srcs:\n",
    "                tmp = chunks[lis].get_kaku_prt() \n",
    "                if len(tmp) > 0 and tmpret != chunks[lis].normalized_surface():\n",
    "                    ret2 += tmp + \" \"\n",
    "                    ret3 += chunks[lis].normalized_surface() + \" \"\n",
    "            print(ret)\n",
    "            print(ret2)\n",
    "            print(ret3)\n",
    "            print(\"--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48.名詞から根へのパスの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re\n",
    "import pydot_ng as pydot\n",
    "\n",
    "fname = 'neko.txt'\n",
    "fname_parsed = 'neko.txt.cabocha'\n",
    "\n",
    "def parse_neko():\n",
    "    with open(fname) as data_file, \\\n",
    "            open(fname_parsed, mode='w') as out_file:\n",
    "\n",
    "        cabocha = CaboCha.Parser()\n",
    "        for line in data_file:\n",
    "            out_file.write(\n",
    "                cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)\n",
    "            )\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "        self.dst = -1\n",
    "\n",
    "    def __str__(self):\n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "\n",
    "    def normalized_surface(self):\n",
    "        result = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                result += morph.surface\n",
    "        return result\n",
    "\n",
    "    def chk_pos(self, pos):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_morphs_by_pos(self,pos,pos1=''):\n",
    "        if len(pos1) > 0:\n",
    "            return [res for res in self.morphs if (res.pos == pos) and (res.pos == pos1)]\n",
    "        else:\n",
    "            return [res for res in self.morphs if res.pos == pos]\n",
    "    \n",
    "    def get_kaku_prt(self):\n",
    "        prts = self.get_morphs_by_pos('助詞')\n",
    "        if len(prts) >1:\n",
    "            kaku_prts = self.get_morphs_by_pos('助詞','格助詞')\n",
    "            if len(kaku_prts) > 0:\n",
    "                prts[-1].surface\n",
    "        \n",
    "        if len(prts) > 0:\n",
    "            return prts[-1].surface\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    def get_sahen_wo(self):\n",
    "        for i,morph in enumerate(self.morphs[0:-1]) :\n",
    "            if(morph.pos == '名詞')\\\n",
    "              and (morph.pos1 == 'サ変接続')\\\n",
    "              and (self.morphs[i+1].pos == \"助詞\")\\\n",
    "              and (self.morphs[i +1].surface == 'を'):\n",
    "                return morph.surface + self.morphs[i + 1].surface\n",
    "        return ''\n",
    "    \n",
    "def neco_lines():\n",
    "    with open(fname_parsed) as file_parsed:\n",
    "        chunks = dict() \n",
    "        idx = -1\n",
    "        for line in file_parsed:\n",
    "            if line == 'EOS\\n':\n",
    "                if len(chunks) > 0:\n",
    "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]\n",
    "                    chunks.clear()\n",
    "                else:\n",
    "                    yield []\n",
    "            elif line[0] == '*':\n",
    "                cols = line.split(' ')\n",
    "                idx = int(cols[1])\n",
    "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "            else:\n",
    "                cols = line.split('\\t')\n",
    "                res_cols = cols[1].split(',')\n",
    "                chunks[idx].morphs.append(\n",
    "                    Morph(\n",
    "                        cols[0],        # surface\n",
    "                        res_cols[6],    # base\n",
    "                        res_cols[0],    # pos\n",
    "                        res_cols[1]     # pos1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "parse_neko()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meishi_pass(chunk,chunks,neko):\n",
    "    if chunk.dst == -1:\n",
    "        neko += chunk.normalized_surface()\n",
    "        print(neko)\n",
    "    else:\n",
    "        for i,x in enumerate(chunks):\n",
    "            if i == chunk.dst:\n",
    "                neko += chunk.normalized_surface() + \"->\"\n",
    "                meishi_pass(x,chunks,neko)\n",
    "        \n",
    "\n",
    "for chunks in neco_lines():\n",
    "    for chunk in chunks:\n",
    "        if chunk.chk_pos('名詞'):\n",
    "            meishi_pass(chunk,chunks,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49.名詞間の係り受けパスの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re\n",
    "import pydot_ng as pydot\n",
    "\n",
    "fname = 'neko.txt'\n",
    "fname_parsed = 'neko.txt.cabocha'\n",
    "\n",
    "def parse_neko():\n",
    "    with open(fname) as data_file, \\\n",
    "            open(fname_parsed, mode='w') as out_file:\n",
    "\n",
    "        cabocha = CaboCha.Parser()\n",
    "        for line in data_file:\n",
    "            out_file.write(\n",
    "                cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)\n",
    "            )\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "        self.dst = -1\n",
    "\n",
    "    def __str__(self):\n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "\n",
    "    def normalized_surface(self):\n",
    "        result = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                result += morph.surface\n",
    "        return result\n",
    "\n",
    "    def chk_pos(self, pos):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_morphs_by_pos(self,pos,pos1=''):\n",
    "        if len(pos1) > 0:\n",
    "            return [res for res in self.morphs if (res.pos == pos) and (res.pos == pos1)]\n",
    "        else:\n",
    "            return [res for res in self.morphs if res.pos == pos]\n",
    "    \n",
    "    def get_kaku_prt(self):\n",
    "        prts = self.get_morphs_by_pos('助詞')\n",
    "        if len(prts) >1:\n",
    "            kaku_prts = self.get_morphs_by_pos('助詞','格助詞')\n",
    "            if len(kaku_prts) > 0:\n",
    "                prts[-1].surface\n",
    "        \n",
    "        if len(prts) > 0:\n",
    "            return prts[-1].surface\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    def get_sahen_wo(self):\n",
    "        for i,morph in enumerate(self.morphs[0:-1]) :\n",
    "            if(morph.pos == '名詞')\\\n",
    "              and (morph.pos1 == 'サ変接続')\\\n",
    "              and (self.morphs[i+1].pos == \"助詞\")\\\n",
    "              and (self.morphs[i +1].surface == 'を'):\n",
    "                return morph.surface + self.morphs[i + 1].surface\n",
    "        return ''\n",
    "    \n",
    "    def noun_masked_surface(self,mask,dst=False):\n",
    "        result = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph .pos != '記号':\n",
    "                if morph.pos == '名詞':\n",
    "                    result += mask\n",
    "                    if dst:    \n",
    "                        return result\n",
    "                    mask = ''\n",
    "                else:\n",
    "                    result += morph.surface\n",
    "        return result\n",
    "    \n",
    "def neco_lines():\n",
    "    with open(fname_parsed) as file_parsed:\n",
    "        chunks = dict() \n",
    "        idx = -1\n",
    "        for line in file_parsed:\n",
    "            if line == 'EOS\\n':\n",
    "                if len(chunks) > 0:\n",
    "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]\n",
    "                    chunks.clear()\n",
    "                else:\n",
    "                    yield []\n",
    "            elif line[0] == '*':\n",
    "                cols = line.split(' ')\n",
    "                idx = int(cols[1])\n",
    "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "            else:\n",
    "                cols = line.split('\\t')\n",
    "                res_cols = cols[1].split(',')\n",
    "                chunks[idx].morphs.append(\n",
    "                    Morph(\n",
    "                        cols[0],        # surface\n",
    "                        res_cols[6],    # base\n",
    "                        res_cols[0],    # pos\n",
    "                        res_cols[1]     # pos1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "parse_neko()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"ruu.txt\",mode='w') as out_file:\n",
    "    for chunks in neco_lines():\n",
    "        index_noun = [i for i in range(len(chunks)) if len(chunks[i].get_morphs_by_pos('名詞'))>0]\n",
    "        if len(index_noun) < 2: continue\n",
    "        for i,index_x in enumerate(index_noun[:-1]):\n",
    "            for index_y in index_noun[i+1:]:\n",
    "                meet_y = False\n",
    "                index_dup = -1\n",
    "                routes_x =set()\n",
    "                dst = chunks[index_x].dst\n",
    "                while dst != -1:\n",
    "                    if dst != index_y:\n",
    "                        meet_y = True\n",
    "                        break\n",
    "                    routes_x.add(dst)\n",
    "                    dst = chunks[dst].dst\n",
    "                if not meet_y:\n",
    "                    dst = chunks[index_y].dst\n",
    "                    while dst != -1:\n",
    "                        if dst in routes_x:\n",
    "                            index_dup =dst\n",
    "                            break\n",
    "                        else:\n",
    "                            dst = chunks[dst].dst\n",
    "            \n",
    "                if index_dup == -1:\n",
    "                    out_file.write(chunks[index_x].noun_masked_surface('X'))\n",
    "                    dst = chunks[index_x].dst\n",
    "                    while dst != -1:\n",
    "                        if dst == index_y:\n",
    "                            out_file.write('->'+chunks[dst].noun_masked_surface('Y',True))\n",
    "                            break\n",
    "                        else:\n",
    "                            out_file.write('->'+chunks[dst].normalized_surface())\n",
    "                        dst = chunks[dst].dst\n",
    "                    out_file.write('\\n')\n",
    "                else:\n",
    "                    out_file.write(chunks[index_x].noun_masked_surface('X'))\n",
    "                    dst = chunks[index_x].dst\n",
    "                    while dst != index_dup:\n",
    "                        out_file.write(' -> ' + chunks[dst].normalized_surface())\n",
    "                        dst = chunks[dst].dst\n",
    "                    out_file.write(' | ')\n",
    "                    # Yからぶつかる手前までを出力\n",
    "                    out_file.write(chunks[index_y].noun_masked_surface('Y'))\n",
    "                    dst = chunks[index_y].dst\n",
    "                    while dst != index_dup:\n",
    "                        out_file.write(' -> ' + chunks[dst].normalized_surface())\n",
    "                        dst = chunks[dst].dst\n",
    "                    out_file.write(' | ')\n",
    "                    # ぶつかったchunkを出力\n",
    "                    out_file.write(chunks[index_dup].normalized_surface())\n",
    "                    out_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
